{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import copy \n",
    "import math \n",
    "from utils import get_smi_list, replace_atom, get_dic, encode_smi, pad_smi, clones, parallel_f, pad, normalize, get_atom_pos, MyDataset\n",
    "from model import SourceAttention, PositionalEncoding, Encoder, TargetAttention\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:33:53] UFFTYPER: Unrecognized atom type: Ba (0)\n"
     ]
    }
   ],
   "source": [
    "smi_list = get_smi_list('data/ADAGRASIB_SMILES.txt')\n",
    "\n",
    "coor_list = parallel_f(get_atom_pos, smi_list)\n",
    "longest_coor = len(max(coor_list, key = len))\n",
    "coor_list = [pad(normalize(c), longest_coor) for c in coor_list]\n",
    "\n",
    "smi_list = [replace_atom(smi) for smi in smi_list]\n",
    "smi_dic = get_dic(smi_list)\n",
    "smint_list = [encode_smi(smi, smi_dic) for smi in smi_list]\n",
    "longest_smint = len(max(smint_list, key = len))\n",
    "smint_list = [pad_smi(smint, longest_smint, smi_dic) for smint in smint_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = MyDataset(smint_list, coor_list)\n",
    "train_set, val_set, test_set = random_split(dataset, [0.9, 0.05, 0.05])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class TargetAttention(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, longest_coor) : \n",
    "        super(TargetAttention, self).__init__() \n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head  \n",
    "    \n",
    "        self.Q = nn.Linear(dim_model, dim_model)   \n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask = None) :\n",
    "        B = Q.size(0) \n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "        print(f'attn_score: {attn_score.shape}')\n",
    "        if mask is not None :\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, num_layer, dropout) : \n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(DecoderLayer(dim_model, num_head, dropout), num_layer)\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "        self.out = nn.Linear(dim_model, 3)\n",
    "    def forward(self, x, target = None) :\n",
    "        for layer in self.layers : \n",
    "            x = layer(x, target) \n",
    "        out = self.out(x)\n",
    "        return out\n",
    "    \n",
    "class DecoderLayer(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, dropout) :\n",
    "        super (DecoderLayer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.norm1 = nn.LayerNorm(dim_model) \n",
    "        self.self_attn = TargetAttention(dim_model, num_head, longest_coor)\n",
    "        self.drop1 = nn.Dropout(dropout) \n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.cross_attn = SourceAttention(dim_model, num_head)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_model, dim_model)\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(dropout) \n",
    "\n",
    "\n",
    "    def forward(self, memory, target) : \n",
    "        x = torch.zeros(memory.size(0), 1, self.dim_model).to(device)\n",
    "        for i in range(1, longest_coor + 1) :\n",
    "            mask = subsequent_mask(i)\n",
    "            mask = mask.unsqueeze(1).to(device)\n",
    "\n",
    "            y = self.norm1(x) \n",
    "\n",
    "            attn, _ = self.self_attn(y, y, y, mask)\n",
    "            y = y + self.drop1(attn) \n",
    "\n",
    "            y = self.norm2(y) \n",
    "            attn, _ = self.cross_attn(y, memory, memory) \n",
    "            y = y + self.drop2(attn) \n",
    "\n",
    "            y = self.norm3(y)\n",
    "            y = y + self.drop3(self.feed_foward(y))\n",
    "            x = torch.cat((x, y[:, -1, :].unsqueeze(1)), dim = 1)\n",
    "        \n",
    "        return y \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory: torch.Size([64, 36, 128])\n",
      "attn_score: torch.Size([64, 2, 1, 1])\n",
      "attn_score: torch.Size([64, 2, 2, 2])\n",
      "attn_score: torch.Size([64, 2, 3, 3])\n",
      "attn_score: torch.Size([64, 2, 4, 4])\n",
      "attn_score: torch.Size([64, 2, 5, 5])\n",
      "attn_score: torch.Size([64, 2, 6, 6])\n",
      "attn_score: torch.Size([64, 2, 7, 7])\n",
      "attn_score: torch.Size([64, 2, 8, 8])\n",
      "attn_score: torch.Size([64, 2, 9, 9])\n",
      "attn_score: torch.Size([64, 2, 10, 10])\n",
      "attn_score: torch.Size([64, 2, 11, 11])\n",
      "attn_score: torch.Size([64, 2, 12, 12])\n",
      "attn_score: torch.Size([64, 2, 13, 13])\n",
      "attn_score: torch.Size([64, 2, 14, 14])\n",
      "attn_score: torch.Size([64, 2, 15, 15])\n",
      "attn_score: torch.Size([64, 2, 16, 16])\n",
      "attn_score: torch.Size([64, 2, 17, 17])\n",
      "attn_score: torch.Size([64, 2, 18, 18])\n",
      "attn_score: torch.Size([64, 2, 19, 19])\n",
      "attn_score: torch.Size([64, 2, 20, 20])\n",
      "attn_score: torch.Size([64, 2, 21, 21])\n",
      "attn_score: torch.Size([64, 2, 22, 22])\n",
      "out: torch.Size([64, 22, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(128, 2, 1, 0.1, len(smi_dic)).to(device)\n",
    "decoder = DecoderLayer(128, 2, 0.1).to(device)\n",
    "\n",
    "for input, target in train_loader :\n",
    "    input = input.to(device)\n",
    "    memory = encoder(input) \n",
    "    print(f'memory: {memory.shape}')\n",
    "    out = decoder(memory, None)\n",
    "    print(f'out: {out.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, layer, N) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "    def forward(self, x, memory, src_mask, tgt_mask) : \n",
    "        for layer in self.layers :\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "c = copy.deepcopy\n",
    "src_vocab = len(smi_dic)\n",
    "\n",
    "class Model(nn.Module) :\n",
    "    def __init__(self,\n",
    "                 dim_model,\n",
    "                 dim_ff,\n",
    "                 num_head,\n",
    "                 dropout,\n",
    "                 N,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 src_embed,\n",
    "                 tgt_embed) :\n",
    "        super(Model, self).__init__()\n",
    "        self.attn = MultiHeadedAttention(num_head, dim_model)\n",
    "        self.ff = PositionwiseFeedForward(dim_model, dim_ff, dropout) \n",
    "        self.position = PositionalEncoding(dim_model, dropout)\n",
    "        self.encoder = Encoder(\n",
    "            EncoderLayer(dim_model, c(self.attn), c(self.ff), dropout), N)\n",
    "        self.decoder = Decoder(\n",
    "            DecoderLayer(dim_model, c(self.attn), c(self.attn), c(self.ff), dropout), N)\n",
    "        \n",
    "        self.src_embed = nn.Sequential(Embeddings(dim_model, src_vocab), c(self.position))\n",
    "        # self.generator = Generator(dim_model) \n",
    "    def forward(self, x) :\n",
    "        src_mask = (x != -2).unsqueeze(-2) \n",
    "        print(f'src_mask : {src_mask.shape}')\n",
    "        x = self.src_embed(x)\n",
    "        print(f'x : {x.shape}')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
