{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import copy \n",
    "import math \n",
    "from utils import get_smi_list, replace_atom, get_dic, encode_smi, pad_smi, clones, parallel_f, pad, normalize, get_atom_pos, MyDataset, subsequent_mask\n",
    "from model import Encoder, Decoder, device\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:57:50] UFFTYPER: Unrecognized atom type: Ba (0)\n"
     ]
    }
   ],
   "source": [
    "smi_list = get_smi_list('data/ADAGRASIB_SMILES.txt')\n",
    "\n",
    "coor_list = parallel_f(get_atom_pos, smi_list)\n",
    "longest_coor = len(max(coor_list, key = len))\n",
    "coor_list = [pad(normalize(c), longest_coor) for c in coor_list]\n",
    "\n",
    "smi_list = [replace_atom(smi) for smi in smi_list]\n",
    "smi_dic = get_dic(smi_list)\n",
    "smint_list = [encode_smi(smi, smi_dic) for smi in smi_list]\n",
    "longest_smint = len(max(smint_list, key = len))\n",
    "smint_list = [pad_smi(smint, longest_smint, smi_dic) for smint in smint_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = MyDataset(smint_list, coor_list)\n",
    "train_set, val_set, test_set = random_split(dataset, [0.9, 0.05, 0.05])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, dropout, longest_coor) :\n",
    "        super (DecoderLayer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.longest_coor = longest_coor\n",
    "\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Linear(3, dim_model),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_model) \n",
    "        self.self_attn = TargetAttention(dim_model, num_head, longest_coor)\n",
    "        self.drop1 = nn.Dropout(dropout) \n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.cross_attn = SourceAttention(dim_model, num_head)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_model, dim_model),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(dropout) \n",
    "\n",
    "\n",
    "    def forward(self, memory, target) : \n",
    "        target = target[:, :-1, :]\n",
    "        mask = subsequent_mask(self.longest_coor - 1)\n",
    "        mask = mask.unsqueeze(1)\n",
    "\n",
    "        target = self.seq1(target) \n",
    "        \n",
    "        target = self.norm1(target) \n",
    "        attn, _ = self.self_attn(target, target, target, mask) \n",
    "        target = target + self.drop1(attn) \n",
    "\n",
    "        target = self.norm2(target) \n",
    "        attn, _ = self.cross_attn(target, memory, memory)\n",
    "        target = target + self.drop2(attn) \n",
    "\n",
    "        target = self.norm3(target) \n",
    "        target = target + self.drop3(self.feed_foward(target)) \n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MODEL = 256 \n",
    "NUM_HEAD = 4\n",
    "NUM_LAYER = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "encoder = Encoder(DIM_MODEL, NUM_HEAD, NUM_LAYER, DROPOUT, len(smi_dic)).to(device)\n",
    "decoder = Decoder(DIM_MODEL, NUM_HEAD, NUM_LAYER, DROPOUT, longest_coor).to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss() \n",
    "e_optim = torch.optim.Adam(encoder.parameters(), lr = 0.001)\n",
    "d_optim = torch.optim.Adam(decoder.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e73323ea0f646889a9141430f3814c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Train Loss: 1.1044 -- Val Loss: 0.0000\n",
      "Epoch 2 -- Train Loss: 0.8625 -- Val Loss: 0.0000\n",
      "Epoch 3 -- Train Loss: 0.8240 -- Val Loss: 0.0000\n",
      "Epoch 4 -- Train Loss: 0.8144 -- Val Loss: 0.0000\n",
      "Epoch 5 -- Train Loss: 0.8013 -- Val Loss: 0.0000\n",
      "Epoch 6 -- Train Loss: 0.7933 -- Val Loss: 0.0000\n",
      "Epoch 7 -- Train Loss: 0.7854 -- Val Loss: 0.0000\n",
      "Epoch 8 -- Train Loss: 0.7801 -- Val Loss: 0.0000\n",
      "Epoch 9 -- Train Loss: 0.7775 -- Val Loss: 0.0000\n",
      "Epoch 10 -- Train Loss: 0.7711 -- Val Loss: 0.0000\n",
      "Epoch 11 -- Train Loss: 0.7677 -- Val Loss: 0.0000\n",
      "Epoch 12 -- Train Loss: 0.7678 -- Val Loss: 0.0000\n",
      "Epoch 13 -- Train Loss: 0.7634 -- Val Loss: 0.0000\n",
      "Epoch 14 -- Train Loss: 0.7627 -- Val Loss: 0.0000\n",
      "Epoch 15 -- Train Loss: 0.7586 -- Val Loss: 0.0000\n",
      "Epoch 16 -- Train Loss: 0.7515 -- Val Loss: 0.0000\n",
      "Epoch 17 -- Train Loss: 0.7560 -- Val Loss: 0.0000\n",
      "Epoch 18 -- Train Loss: 0.7495 -- Val Loss: 0.0000\n",
      "Epoch 19 -- Train Loss: 0.7488 -- Val Loss: 0.0000\n",
      "Epoch 20 -- Train Loss: 0.7444 -- Val Loss: 0.0000\n",
      "Epoch 21 -- Train Loss: 0.7414 -- Val Loss: 0.0000\n",
      "Epoch 22 -- Train Loss: 0.7455 -- Val Loss: 0.0000\n",
      "Epoch 23 -- Train Loss: 0.7358 -- Val Loss: 0.0000\n",
      "Epoch 24 -- Train Loss: 0.7340 -- Val Loss: 0.0000\n",
      "Epoch 25 -- Train Loss: 0.7311 -- Val Loss: 0.0000\n",
      "Epoch 26 -- Train Loss: 0.7312 -- Val Loss: 0.0000\n",
      "Epoch 27 -- Train Loss: 0.7292 -- Val Loss: 0.0000\n",
      "Epoch 28 -- Train Loss: 0.7322 -- Val Loss: 0.0000\n",
      "Epoch 29 -- Train Loss: 0.7251 -- Val Loss: 0.0000\n",
      "Epoch 30 -- Train Loss: 0.7256 -- Val Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30 \n",
    "\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS +1), total=NUM_EPOCHS) : \n",
    "    train_loss = 0 \n",
    "    val_loss = 0\n",
    "    encoder.train(), decoder.train()\n",
    "    for input, target in train_loader :\n",
    "        input, target = input.to(device), target.to(device) \n",
    "        memory = encoder(input) \n",
    "        prediction = decoder(memory, target)\n",
    "\n",
    "        loss = loss_fn(prediction, target[:, 1:, :]) \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        e_optim.step(), d_optim.step()\n",
    "        e_optim.zero_grad(), d_optim.zero_grad() \n",
    "\n",
    "    # encoder.eval(), decoder.eval()\n",
    "    # with torch.no_grad() :\n",
    "    #     for input, target in val_loader :\n",
    "    #         input, target = input.to(device), target.to(device) \n",
    "    #         memory = encoder(input) \n",
    "    #         prediction = decoder(memory, None)\n",
    "\n",
    "    #         loss = loss_fn(prediction, target) \n",
    "    #         val_loss += loss.item()\n",
    "    print(f'Epoch {epoch} -- Train Loss: {train_loss / len(train_loader):.4f} -- Val Loss: {val_loss / len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
